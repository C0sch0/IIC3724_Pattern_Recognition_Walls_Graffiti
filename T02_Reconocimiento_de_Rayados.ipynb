{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reconocimiento de Patrones en ImÃ¡genes**\n",
    "\n",
    "Se cuenta con una base de datos de 10.000 patches a color de 64x64 pixeles, correspondientes a porciones de paredes que han sido y que no han sido rayadas, distribuidas 50-50%, es decir 5.000 patches pertencientes a la clase 1 (rayas), y 5.000 patches pertenecientes a la clase 0 (no-rayas).\n",
    "\n",
    "Cada uno de estos patches cubre aproximadamente una superficie de 30cm x 30cm de la pared.\n",
    "\n",
    "Se debe disenar un clasificador que funcione con un maximo de 50 caraceristicas, para esto se deben sacar al menos 200 caracteristica y a partir de tecnicas se seleccion o transformacion de caracteristicas se le debe proporcionar al clasificador un maximo de 50 caracteristicas. \n",
    "\n",
    "El clasificador a emplear es un KNN de tres vecinos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import sys\n",
    "import numpy as np\n",
    "from os import listdir\n",
    "import fnmatch, os\n",
    "\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import tqdm\n",
    "from PIL import Image\n",
    "from seaborn import heatmap\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from itertools import combinations\n",
    "from pybalu.feature_extraction import lbp_features, hog_features, gabor_features, haralick_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imageio import imread as _imread\n",
    "\n",
    "def imread(filename, *, normalize=False, flatten=False):\n",
    "    img = _imread(filename)\n",
    "    if flatten:\n",
    "        img = img @ [0.299, 0.587, 0.114]\n",
    "    if normalize:\n",
    "        return (img / 255)\n",
    "    return img.astype(_np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esta es la funcion de Pybalu, permitida usarla segun enunciado\n",
    "def clean(features, show=False):\n",
    "    n_features = features.shape[1]\n",
    "    ip = np.ones(n_features, dtype=int)\n",
    "\n",
    "    # cleaning correlated features\n",
    "    warnings.filterwarnings('ignore')\n",
    "    C = np.abs(np.corrcoef(features, rowvar=False))\n",
    "    idxs = np.vstack(np.where(C > .99))\n",
    "    \n",
    "    # remove pairs of same feature ( feature i will have a correlation of 1 whit itself )\n",
    "    idxs = idxs[:, idxs[0,:] != idxs[1,:]]\n",
    "    \n",
    "    # remove correlated features\n",
    "    if idxs.size > 0:\n",
    "        ip[np.max(idxs, 0)] = 0\n",
    "    \n",
    "    # remove constant features\n",
    "    s = features.std(axis=0, ddof=1)\n",
    "    ip[s < 1e-8] = 0\n",
    "    p = np.where(ip.astype(bool))[0]\n",
    "\n",
    "    if show:\n",
    "        print(f'Clean: number of features reduced from {n_features} to {p.size}.')\n",
    "\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jfisher(features, classification, p=None): \n",
    "    m = features.shape[1]\n",
    "    \n",
    "    norm = classification.ravel() - classification.min()\n",
    "    max_class = norm.max() + 1\n",
    "    \n",
    "    if p is None:\n",
    "        p = np.ones(shape=(max_class, 1)) / max_class\n",
    "        \n",
    "    features_mean = features.mean(0)\n",
    "    cov_w = np.zeros(shape=(m, m))\n",
    "    cov_b = np.zeros(shape=(m, m))\n",
    "\n",
    "    for k in range(max_class):\n",
    "        ii = (norm == k)                                  \n",
    "        class_features = features[ii,:]                    \n",
    "        class_mean = class_features.mean(0)                \n",
    "        class_cov = np.cov(class_features, rowvar=False)   \n",
    "        \n",
    "        cov_w += p[k] * class_cov                         \n",
    "        \n",
    "        dif = (class_mean - features_mean).reshape((m, 1))\n",
    "        cov_b += p[k] * dif @ dif.T                 \n",
    "    try:\n",
    "        return np.trace(np.linalg.inv(cov_w) @ cov_b)\n",
    "    except np.linalg.LinAlgError:\n",
    "        return - np.inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraccion de caracteristicas\n",
    "# Probamos con Gabor, Haralich, HOG, LBP, LBP por gama de colores y escala de grises\n",
    "\n",
    "def imshow(image):\n",
    "    pil_image = Image.fromarray(image)\n",
    "    pil_image.show()\n",
    "\n",
    "def get_image(path, show=False):\n",
    "    img = cv2.imread(path)\n",
    "    if show:\n",
    "        imshow(img)\n",
    "    return img\n",
    "\n",
    "def dirfiles(img_path,img_ext):\n",
    "    img_names = fnmatch.filter(sorted(os.listdir(img_path)),img_ext)\n",
    "    return img_names\n",
    "\n",
    "\n",
    "def num2fixstr(x,d):\n",
    "    st = '%0*d' % (d,x)\n",
    "    return st\n",
    "\n",
    "def extract_features(dirpath,fmt):\n",
    "    \n",
    "    st = '*.'+fmt\n",
    "    img_names = dirfiles(dirpath+'/',st)\n",
    "    n = len(img_names)\n",
    "    print(n)\n",
    "    for i in range(n):\n",
    "        img_path = img_names[i]\n",
    "        img      = get_image(dirpath+'/'+img_path)\n",
    "        escala_grises     = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        X_0      = lbp_features(escala_grises, hdiv=1, vdiv=1, mapping='nri_uniform')\n",
    "        rojo     = lbp_features(img[:,:,0], hdiv=1, vdiv=1, mapping='nri_uniform')\n",
    "        verde   = lbp_features(img[:,:,1], hdiv=1, vdiv=1, mapping='nri_uniform')\n",
    "        azul    = lbp_features(img[:,:,2], hdiv=1, vdiv=1, mapping='nri_uniform')\n",
    "        # Haralick = haralick_features(img.astype(int))\n",
    "        # hog = hog_features(i, v_windows=3, h_windows=3, n_bins=8)\n",
    "        features = np.asarray(np.concatenate((X_0, rojo, verde, azul)))\n",
    "        \n",
    "        if i==0:\n",
    "            m = features.shape[0]\n",
    "            data = np.zeros((n,m))\n",
    "            print('size of extracted features:')\n",
    "            print(features.shape)\n",
    "        data[i]    = features\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para aprender como usar SFS, vimos el codigo base de pybalu, pero reemplazamos todas las dependencias \n",
    "# dentro de la libreria por metodos propios.\n",
    "\n",
    "def Sequential_Feature_Selector(features, classes, n_features):\n",
    "    remaining_feats = set(np.arange(features.shape[1]))\n",
    "    selected = list()\n",
    "    curr_feats = np.zeros((features.shape[0], 0))\n",
    "    options = dict()\n",
    "\n",
    "    def fisher_score(i):\n",
    "        feats = np.hstack([curr_feats, features[:, i].reshape(-1, 1)])\n",
    "        m = features.shape[1]\n",
    "        norm = classes.ravel() - classes.min()\n",
    "        max_class = norm.max() + 1\n",
    "        p = np.ones(shape=(max_class, 1)) / max_class\n",
    "\n",
    "        features_mean = feats.mean(0)\n",
    "        cov_w = np.zeros(shape=(m, m))\n",
    "        cov_b = np.zeros(shape=(m, m))\n",
    "\n",
    "        for k in range(max_class):\n",
    "            ii = (norm == k)                                  \n",
    "            class_features = features[ii,:]                    \n",
    "            class_mean = class_features.mean(0)                \n",
    "            class_cov = np.cov(class_features, rowvar=False)   \n",
    "\n",
    "            cov_w += p[k] * class_cov                         \n",
    "\n",
    "            dif = (class_mean - features_mean).reshape((m, 1))\n",
    "            cov_b += p[k] * dif @ dif.T                 \n",
    "        try:\n",
    "            return np.trace(np.linalg.inv(cov_w) @ cov_b)\n",
    "        except np.linalg.LinAlgError:\n",
    "            return - np.inf\n",
    "        \n",
    "\n",
    "    _range = tqdm.trange(n_features, desc='', unit_scale=True, unit=' features')\n",
    "\n",
    "    for _ in _range:\n",
    "        new_selected = max(remaining_feats, key=fisher_score)\n",
    "        selected.append(new_selected)\n",
    "        remaining_feats.remove(new_selected)\n",
    "        curr_feats = np.hstack([curr_feats, features[:, new_selected].reshape(-1, 1)])\n",
    "\n",
    "    return np.array(selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Si bien lo implementamos, no lo usaremos, ya que demostro no aportar a nuestro accuracy de prediccion\n",
    "# El uso de numpy inspirado por https://sebastianraschka.com/Articles/2014_pca_step_by_step.html\n",
    "\n",
    "def PCA(data, n_componentes):\n",
    "    # seleccionadas = [columnas] #las columnas que queremos usar para el pca. Data debe ser un DF de pandas\n",
    "    \n",
    "    # x = data.loc[:, seleccionadas].values  Sin label !!!!!\n",
    "    # y = data.loc[:, [labels]].values  \n",
    "    \n",
    "    # primero se saca la matriz de covarianza, usando la media y la desviacion estandar de los datos.\n",
    "    # Luego sacamos los eigen vectors\n",
    "\n",
    "    # Ordenamos los eigen vectors \n",
    "    pares = list(zip(np.linalg.eig(np.cov((x - x.mean()) / (x.std()).T) )[0], np.linalg.eig(np.cov((x - x.mean()) / (x.std()).T) )[1]))\n",
    "    pares.sort(key=lambda vector: vector[0], reverse=True)\n",
    "    vectores_seleccionados = []\n",
    "    \n",
    "    # sacamos los N primeros\n",
    "    for component in range(columnas):\n",
    "        vectores_seleccionados.append(pares[component][1])  \n",
    "            \n",
    "    matriz_scatter = np.hstack(map(lambda vector: vectores_seleccionados.reshape(len(seleccionadas), 1), vectores_seleccionados))\n",
    "    return pd.concat([pd.DataFrame(data = (x - x.mean()) / (x.std()).dot(matriz_scatter), columns=[f'PC{componente}' for componente in range(1, n_componentes + 1)]), data[['char']]], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_Train_clean_walls = [picture for picture in listdir('./Training_0') if picture.endswith('png')]\n",
    "x_Train_intervened_walls = [picture for picture in listdir('./Training_1') if picture.endswith('png')]\n",
    "Y_Testing_clean_walls = [picture for picture in listdir('./Testing_0') if picture.endswith('png')]\n",
    "y_Testing_intervened_walls = [picture for picture in listdir('./Testing_1') if picture.endswith('png')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean Train: 4000, Intervened Train:4000, Clean Test:1000, Intervened Test:1000\n"
     ]
    }
   ],
   "source": [
    "print(\"Clean Train: {0}, Intervened Train:{1}, Clean Test:{2}, Intervened Test:{3}\".format(len(X_Train_clean_walls), \n",
    "                                                                                           len(x_Train_intervened_walls), \n",
    "                                                                                           len(Y_Testing_clean_walls), \n",
    "                                                                                           len(y_Testing_intervened_walls)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000\n",
      "size of extracted features:\n",
      "(236,)\n",
      "4000\n",
      "size of extracted features:\n",
      "(236,)\n",
      "1000\n",
      "size of extracted features:\n",
      "(236,)\n",
      "1000\n",
      "size of extracted features:\n",
      "(236,)\n"
     ]
    }
   ],
   "source": [
    "X0_train = extract_features('Training_0','png')\n",
    "X1_train = extract_features('Training_1','png')\n",
    "X0_test  = extract_features('Testing_0','png')\n",
    "X1_test  = extract_features('Testing_1','png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Subset:\n",
      "Original extracted features: 236(8000 samples)\n"
     ]
    }
   ],
   "source": [
    "print('Training Subset:')\n",
    "X_train  = np.concatenate((X0_train, X1_train),axis=0)\n",
    "d0_train = np.zeros([X0_train.shape[0], 1],dtype=int)\n",
    "d1_train = np.ones([X1_train.shape[0], 1],dtype=int)\n",
    "d_train  = np.concatenate((d0_train, d1_train),axis=0)\n",
    "print('Original extracted features: '+str(X_train.shape[1])+ '('+str(X_train.shape[0])+' samples)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean: number of features reduced from 236 to 212.\n",
      "cleaned features: 212(8000 samples)\n"
     ]
    }
   ],
   "source": [
    "# Eliminamos caracteristicas altamente correlacionadas o constantes para set de Training\n",
    "sclean = clean(X_train,show=True)\n",
    "X_train_clean = X_train[:,sclean]\n",
    "print('cleaned features: '+str(X_train_clean.shape[1])+ '('+str(X_train_clean.shape[0])+' samples)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalized features: 212(8000 samples)\n"
     ]
    }
   ],
   "source": [
    "# Normalizamos las columnas de datos\n",
    "X_train_norm = X_train_clean * (1 / X_train_clean.std(0)) + (- X_train_clean.mean(0) / X_train_clean.std(0))\n",
    "\n",
    "print('normalized features: '+str(X_train_norm.shape[1])+ '('+str(X_train_norm.shape[0])+' samples)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training: Feature selection\n",
    "    ssfs = Sequential_Feature_Selector(X_train_norm, d_train, n_features=80)\n",
    "X_train_sfs = X_train_norm[:,ssfs]\n",
    "print('selected features: '+str(X_train_sfs.shape[1])+ '('+str(X_train_sfs.shape[0])+' samples)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing dataset\n",
    "print('Testing Subset:')\n",
    "X_test  = np.concatenate((X0_test,X1_test),axis=0)\n",
    "d0_test = np.zeros([X0_test.shape[0],1],dtype=int)\n",
    "d1_test = np.ones([X1_test.shape[0],1],dtype=int)\n",
    "d_test  = np.concatenate((d0_test,d1_test),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing: Cleaning\n",
    "X_test_clean = X_test[:,sclean]\n",
    "\n",
    "# Testing: Normalization\n",
    "X_test_norm = X_test_clean * (1 / X_train_clean.std(0)) + (- X_train_clean.mean(0) / X_train_clean.std(0))\n",
    "\n",
    "# Testing: Feature selection\n",
    "X_test_sfs = X_test_norm[:,ssfs]\n",
    "print('clean+norm+sfs features: '+str(X_test_sfs.shape[1])+ '('+str(X_test_sfs.shape[0])+' samples)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification on Testing dataset\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "knn.fit(X_train_sfs, d_train)\n",
    "\n",
    "acc = accuracy_score(d_test,knn.predict(X_test_sfs))\n",
    "print('Confusion Matrix:')\n",
    "print(confusion_matrix(d_test,knn.predict(X_test_sfs)) )\n",
    "print('Accuracy = '+str(acc))\n",
    "print()\n",
    "\n",
    "\n",
    "plt.figure(figsize=(7,5))\n",
    "heatmap(confusion_matrix(d_test,knn.predict(X_test_sfs)) , annot=True, fmt=\"d\", cmap=\"YlGnBu\")\n",
    "plt.xlim(0,confusion_matrix(d_test,knn.predict(X_test_sfs)).shape[0])\n",
    "plt.ylim(confusion_matrix(d_test,knn.predict(X_test_sfs)).shape[0],0)\n",
    "plt.title('Confusion Matrix Testing',fontsize=14)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_sk = PCA(n_components=19)\n",
    "X_train_pcaed = pca_sk.fit_transform(X_train_sfs)\n",
    "X_test_pcaed = pca_sk.transform(X_test_sfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification on Testing dataset\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "knn.fit(X_train_pcaed, d_train)\n",
    "ds = knn.predict(X_test_pcaed)\n",
    "Accuracy(ds, d_test,'Testing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "100: 0.9555\n",
    "85:  0.9605\n",
    "81:  0.96\n",
    "80:  0.9635\n",
    "79:  0.9595\n",
    "76:  0.9575\n",
    "75:  0.9575\n",
    "70:  0.957\n",
    "60:  0.955\n",
    "50:  0.957\n",
    "30:  0.94\n",
    "20:  0.935"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
